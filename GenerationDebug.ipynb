{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/drumloop2/drumloop2.keras\"\n",
    "maxlen = 8\n",
    "prompt = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "sequence_length = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"drumloop2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_17 (InputLayer)       [(None, 8, 4)]            0         \n",
      "                                                                 \n",
      " gru_16 (GRU)                (None, 24)                2160      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 4)                 100       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,260\n",
      "Trainable params: 2,260\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "global model\n",
    "model = load_model(model_path)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = preds[0]\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    # print(preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt=[], length=4, temperature=1.0, include_prompt=False, verbose=0):\n",
    "    # generate a sequence of given length       \n",
    "    if len(prompt) == 0:\n",
    "        # if it's there's no prompt, use an array of zeroes\n",
    "        prompt = np.zeros((maxlen), dtype=\"uint8\")\n",
    "        print(f\"No prompt provided, using {maxlen} zeroes instead.\")\n",
    "    elif len(prompt) < maxlen:\n",
    "        # if prompt is too short, pad it with zeroes from the left to match correct input shape\n",
    "        prompt = np.pad(prompt, (maxlen - len(prompt), 0), 'constant', constant_values=(0, 0))\n",
    "        print(\"Prompt too short, padded to length: \", maxlen)\n",
    "    elif len(prompt) > maxlen:\n",
    "        # if it's too long, then trim it\n",
    "        prompt = prompt[-maxlen:]\n",
    "        print(f\"Prompt too long, using {maxlen} last elements: \")\n",
    "    prompt_ = to_categorical(prompt, num_classes=4)\n",
    "    prompt_ = np.array([prompt_])\n",
    "    \n",
    "    seq = []\n",
    "    for i in range(length):\n",
    "        # make prediction based on prompt\n",
    "        ps = model.predict(prompt_, verbose=verbose)\n",
    "        # sample from predictions\n",
    "        p_label = sample(ps, temperature)\n",
    "        # add sampled label to sequence\n",
    "        seq.extend([int(p_label)])\n",
    "        # one-hot encode sampled label\n",
    "        p_label_ = to_categorical(p_label, num_classes=4)\n",
    "        p_label_ = np.array([[p_label_]])\n",
    "        # append encoded label to the prompt for next prediction\n",
    "        prompt_ = np.append(prompt_, p_label_, axis=1)\n",
    "    if include_prompt:\n",
    "        prompt = np.append(prompt, seq)\n",
    "        return prompt\n",
    "    else:\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq = generate(prompt=prompt, length=sequence_length)\n",
    "# seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_thread():\n",
    "    global prompt\n",
    "    while True:\n",
    "        print(\"Generating sequence...\")\n",
    "        seq = generate(prompt=prompt, length=sequence_length)\n",
    "        prompt.extend(seq) # add generated sequence to the prompt\n",
    "        prompt = prompt[-maxlen:] # take only last maxlen elements\n",
    "        print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
