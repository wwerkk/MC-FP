{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"audio-data/drumloop.mp3\"\n",
    "name = \"drumloop-ae\"\n",
    "\n",
    "# segmentation\n",
    "block_length = 1 # number of frames in a stream block\n",
    "BPM = 120\n",
    "beat = 1 / 8 # metered divisions\n",
    "hop_beats = 1 / 8 # hop length in beats\n",
    "## alternatively, you can specify frame length and hop length in seconds\n",
    "# frame_length_s = 0.1\n",
    "# hop_length_s = 0.02\n",
    "## or in samples\n",
    "# frame_length = 4096\n",
    "# hop_length = 1024\n",
    "\n",
    "# clustering\n",
    "# model architecture and training\n",
    "step = 2 # step size for sliding window\n",
    "hidden_units = 24 # number of hidden units in GRU layer\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "validation_split = 0.2\n",
    "patience = 15 # number of epochs to wait before early stopping\n",
    "\n",
    "# utility\n",
    "verbose = 1 # print additional information\n",
    "directory = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import math\n",
    "import numpy as np, matplotlib.pyplot as plt, librosa\n",
    "import librosa.display\n",
    "# import IPython.display\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import GRU\n",
    "from keras.callbacks import EarlyStopping\n",
    "import json\n",
    "from numpyencoder import NumpyEncoder\n",
    "from sklearn import preprocessing, cluster\n",
    "from streamer import Streamer\n",
    "import scipy\n",
    "\n",
    "if BPM is None and beat is not None or BPM is not None and beat is None:\n",
    "    print(\"Please specify both BPM and beat if you are using metered divisions.\")\n",
    "    exit()\n",
    "frame_length_s = (4 * beat * 60 / BPM) if BPM is not None else args.frame_length_s\n",
    "hop_length_s = (4 * hop_beats * 60 / BPM) if BPM is not None else args.hop_length_s\n",
    "sr = librosa.get_samplerate(audio_path)\n",
    "frame_length = math.ceil(frame_length_s * sr) if frame_length_s is not None else args.frame_length\n",
    "hop_length = math.ceil(hop_length_s * sr) if hop_length_s is not None else args.hop_length\n",
    "stream = Streamer(audio_path, block_length, frame_length, hop_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral Centroid \tThe center of gravity of the spectrum.\n",
    "# 5 \tSpectral Spread \tThe second central moment of the spectrum.\n",
    "# 6 \tSpectral Entropy \tEntropy of the normalized spectral energies for a set of sub-frames.\n",
    "# 7 \tSpectral Flux \tThe squared difference between the normalized magnitudes of the spectra of the two successive frames.\n",
    "# 8 \tSpectral Rolloff \tThe frequency below which 90% of the magnitude distribution of the spectrum is concentrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# helper function to extract features from audio block\n",
    "def extract_features(y, sr):\n",
    "    zcr = [librosa.zero_crossings(y).sum()]\n",
    "    energy = [scipy.linalg.norm(y)]\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spectral_bandwith = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "    spectral_flatness = librosa.feature.spectral_flatness(y=y)\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "    m_centroid = np.median(spectral_centroid, axis=1)\n",
    "    m_bandwith = np.median(spectral_bandwith, axis=1)\n",
    "    m_flatness = np.median(spectral_flatness, axis=1)\n",
    "    m_rolloff = np.median(spectral_rolloff, axis=1)\n",
    "    if y.size >= 2048:  \n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, center=False) # mfccs\n",
    "    else:\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, n_fft=len(y), hop_length=len(y), center=False)\n",
    "    \n",
    "    m_mfccs = np.median(mfccs, axis=1)\n",
    "    if m_mfccs.size == 0:\n",
    "        print(\"Empty frame!\")\n",
    "        return None\n",
    "    else:        \n",
    "        features = np.concatenate((zcr, energy, m_centroid, m_bandwith, m_flatness, m_rolloff, m_mfccs))\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio length: 128.016s, 6144768 samples\n",
      "Sample rate: 48000 Hz\n",
      "Frame length: 0.25s,  12000 samples\n",
      "Hop length: 0.25s, 12000 samples\n",
      "Block length: 1 frame(s)\n",
      "Number of blocks: 513\n",
      "(513, 19)\n",
      "(513, 19)\n",
      "[-0.33685137 -0.41398088  0.23415425  0.23415425 -0.419142    0.11576887\n",
      " -4.2425808  -0.40418529 -0.43415831 -0.40965724 -0.42710094 -0.40814716\n",
      " -0.41349551 -0.40640078 -0.41207224 -0.4074899  -0.41267933 -0.40764511\n",
      " -0.41455116]\n",
      "[ 0.89446114  0.23426025  1.51580417  3.49427288  0.23811116  3.54006506\n",
      " -0.55510719  0.23994505  0.23415425  0.23415425  0.23415425  0.23415425\n",
      "  0.23415425  0.23415425  0.23415425  0.23415425  0.23415425  0.23415425\n",
      "  0.23415425]\n",
      "[ 0.89446114 -0.23703537  0.29853695  3.31132959 -0.27830617  0.11576887\n",
      " -2.57420888  0.17051861  0.03896163 -0.02986914 -0.10245753 -0.1484586\n",
      " -0.16675341 -0.18349676 -0.20101659 -0.21369144 -0.22534009 -0.23204122\n",
      " -0.23690159]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"Audio length: {stream.length}s, {stream.n_samples} samples\")\n",
    "print(f\"Sample rate: {sr} Hz\")\n",
    "print(f\"Frame length: {frame_length_s}s,  {frame_length} samples\")\n",
    "print(f\"Hop length: {hop_length_s}s, {hop_length} samples\")\n",
    "print(f\"Block length: {block_length} frame(s)\")\n",
    "print(f\"Number of blocks: {len(stream)}\")\n",
    "\n",
    "# extract features from each block in audio stream\n",
    "features = np.array([extract_features(block, sr) for block in stream.new()])\n",
    "features_scaled = preprocessing.scale(features, axis=1)\n",
    "if verbose:\n",
    "    # print(features[0])\n",
    "    print(features.shape)\n",
    "    print(features_scaled.shape)\n",
    "    print(features_scaled.min(axis=0))\n",
    "    print(features_scaled.max(axis=0))\n",
    "    print(features_scaled[0]) # type: ignore\n",
    "if verbose:\n",
    "    plt.scatter(features_scaled[:,0], features_scaled[:,1], ) # type: ignore\n",
    "    # plt.xlabel('MFCC0 (scaled)')\n",
    "    # plt.ylabel('MFCC1 Centroid (scaled)')   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 19)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = features_scaled[:10]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 19)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 9, 19)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0294\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.9927\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.9579\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.9232\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.8870\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.8468\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.8011\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.7468\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.6812\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.6046\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.5266\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.5015\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.5434\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.4799\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.4244\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.4069\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.4058\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.4060\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.4022\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.3946\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.3854\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.3774\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.3717\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.3666\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.3589\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.3485\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.3380\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.3296\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.3233\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.3181\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.3134\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.3085\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.3034\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.2983\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.2934\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.2887\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.2840\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.2793\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.2744\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.2698\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.2659\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.2627\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.2597\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.2571\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.2546\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.2523\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.2502\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.2480\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.2458\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.2440\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.2421\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.2402\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.2383\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.2365\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.2348\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.2333\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.2317\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.2304\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.2290\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.2274\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.2259\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.2244\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.2229\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.2214\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.2199\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.2185\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.2170\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.2154\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.2137\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.2120\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.2103\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.2086\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.2069\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.2052\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.2035\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.2017\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.1999\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.1981\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 0.1963\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.1945\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.1926\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.1908\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.1889\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.1870\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.1850\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.1831\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.1812\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.1792\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.1772\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.1751\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.1728\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.1705\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.1679\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.1651\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.1616\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.1578\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.1532\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.1479\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.1421\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.1374\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x2bf0d1240> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 200ms/step\n",
      "[[[ 1.2864344  -0.19758976  0.22784278  4.021671   -0.62029165\n",
      "    0.22261855 -3.1923573   0.17660795  0.05810229 -0.00652567\n",
      "   -0.02704872 -0.01213004 -0.11660396 -0.15580559  0.044831\n",
      "   -0.12303618 -0.26355705 -0.3252981  -0.36174312]\n",
      "  [-0.48533517 -0.35593224  1.300436    0.95810676  0.01300508\n",
      "    3.085816   -0.01180083 -0.50787103 -0.5038781  -0.4469477\n",
      "   -0.6765906  -0.4475043  -0.48508143 -0.3118635  -0.7470325\n",
      "   -0.29166284 -0.23528242  0.08919138 -0.22974817]\n",
      "  [ 0.17197698 -0.26656854  0.9199462   3.2969942  -0.7410288\n",
      "    1.8599207  -1.6911094  -0.06463636 -0.21259095 -0.22706072\n",
      "   -0.17017417 -0.4242166  -0.25278696 -0.38727343 -0.10231247\n",
      "   -0.48417643 -0.44714606 -0.48343396 -0.58676714]\n",
      "  [-0.4171507  -0.39057785  1.2226102   1.5155826  -0.13697837\n",
      "    2.9368749  -0.41541556 -0.40839502 -0.40781882 -0.39252728\n",
      "   -0.55325544 -0.38186565 -0.44983992 -0.40843907 -0.62629837\n",
      "   -0.39517042 -0.29967672 -0.1549793  -0.32872847]\n",
      "  [-0.01611521 -0.28953758  0.9921107   2.912772   -0.5675359\n",
      "    2.112127   -1.4691888  -0.08331092 -0.25523362 -0.24083638\n",
      "   -0.20595942 -0.42340192 -0.30288047 -0.40231752 -0.24189067\n",
      "   -0.45542175 -0.41514164 -0.50093687 -0.5116434 ]\n",
      "  [-0.2870612  -0.38548872  1.1251884   1.8883047  -0.17413634\n",
      "    2.600985   -0.8382823  -0.28346175 -0.35100016 -0.32847902\n",
      "   -0.44133174 -0.3319094  -0.3735853  -0.35843077 -0.5518313\n",
      "   -0.31075367 -0.30278522 -0.24333349 -0.3051561 ]\n",
      "  [-0.02404957 -0.32062632  0.92683166  2.767012   -0.45616278\n",
      "    1.9437742  -1.5314604  -0.0353299  -0.22685061 -0.21825778\n",
      "   -0.19325157 -0.3310592  -0.25460136 -0.30876502 -0.30709866\n",
      "   -0.3464884  -0.36509395 -0.42511213 -0.48051417]\n",
      "  [-0.0810945  -0.38730472  0.8977728   2.2052171  -0.25967747\n",
      "    1.9531758  -1.272748   -0.11579217 -0.18856959 -0.25455534\n",
      "   -0.28446746 -0.18968877 -0.22868285 -0.2457605  -0.45643193\n",
      "   -0.2678875  -0.27539313 -0.21662965 -0.4035148 ]\n",
      "  [ 0.17782737 -0.3560871   0.655218    2.8048675  -0.49671292\n",
      "    1.2391412  -1.8322055   0.10485457 -0.00992225 -0.15297806\n",
      "   -0.06432448 -0.10866537 -0.09330364 -0.18075138 -0.26765507\n",
      "   -0.3410354  -0.2665863  -0.27105    -0.6046138 ]]]\n"
     ]
    }
   ],
   "source": [
    "# lstm autoencoder reconstruct and predict sequence\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "from keras import Sequential\n",
    "# define input sequence\n",
    "seq_in = x\n",
    "# reshape input into [samples, timesteps, features]\n",
    "n_in = len(seq_in)\n",
    "seq_in = seq_in.reshape((1, n_in, 19))\n",
    "# prepare output sequence\n",
    "seq_out = seq_in[:, 1:, :]\n",
    "n_out = n_in - 1\n",
    "seq_in = seq_in[:, :-1, :]\n",
    "n_in = len(seq_in)\n",
    "# define encoder\n",
    "visible = Input(shape=(9,19))\n",
    "encoder = LSTM(100, activation='relu')(visible)\n",
    "# define predict decoder\n",
    "decoder2 = RepeatVector(n_out)(encoder)\n",
    "decoder2 = LSTM(100, activation='relu', return_sequences=True)(decoder2)\n",
    "decoder2 = TimeDistributed(Dense(19))(decoder2)\n",
    "# tie it together\n",
    "model = Model(inputs=visible, outputs=decoder2)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "plot_model(model, show_shapes=True, to_file='composite_lstm_autoencoder.png')\n",
    "# fit model\n",
    "model.fit(seq_in, [seq_in,seq_out], epochs=100, verbose=1)\n",
    "# demonstrate prediction\n",
    "yhat = model.predict(seq_in, verbose=1)\n",
    "print(yhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.2864344  -0.19758976  0.22784278  4.021671   -0.62029165\n",
      "    0.22261855 -3.1923573   0.17660795  0.05810229 -0.00652567\n",
      "   -0.02704872 -0.01213004 -0.11660396 -0.15580559  0.044831\n",
      "   -0.12303618 -0.26355705 -0.3252981  -0.36174312]\n",
      "  [-0.48533517 -0.35593224  1.300436    0.95810676  0.01300508\n",
      "    3.085816   -0.01180083 -0.50787103 -0.5038781  -0.4469477\n",
      "   -0.6765906  -0.4475043  -0.48508143 -0.3118635  -0.7470325\n",
      "   -0.29166284 -0.23528242  0.08919138 -0.22974817]\n",
      "  [ 0.17197698 -0.26656854  0.9199462   3.2969942  -0.7410288\n",
      "    1.8599207  -1.6911094  -0.06463636 -0.21259095 -0.22706072\n",
      "   -0.17017417 -0.4242166  -0.25278696 -0.38727343 -0.10231247\n",
      "   -0.48417643 -0.44714606 -0.48343396 -0.58676714]\n",
      "  [-0.4171507  -0.39057785  1.2226102   1.5155826  -0.13697837\n",
      "    2.9368749  -0.41541556 -0.40839502 -0.40781882 -0.39252728\n",
      "   -0.55325544 -0.38186565 -0.44983992 -0.40843907 -0.62629837\n",
      "   -0.39517042 -0.29967672 -0.1549793  -0.32872847]\n",
      "  [-0.01611521 -0.28953758  0.9921107   2.912772   -0.5675359\n",
      "    2.112127   -1.4691888  -0.08331092 -0.25523362 -0.24083638\n",
      "   -0.20595942 -0.42340192 -0.30288047 -0.40231752 -0.24189067\n",
      "   -0.45542175 -0.41514164 -0.50093687 -0.5116434 ]\n",
      "  [-0.2870612  -0.38548872  1.1251884   1.8883047  -0.17413634\n",
      "    2.600985   -0.8382823  -0.28346175 -0.35100016 -0.32847902\n",
      "   -0.44133174 -0.3319094  -0.3735853  -0.35843077 -0.5518313\n",
      "   -0.31075367 -0.30278522 -0.24333349 -0.3051561 ]\n",
      "  [-0.02404957 -0.32062632  0.92683166  2.767012   -0.45616278\n",
      "    1.9437742  -1.5314604  -0.0353299  -0.22685061 -0.21825778\n",
      "   -0.19325157 -0.3310592  -0.25460136 -0.30876502 -0.30709866\n",
      "   -0.3464884  -0.36509395 -0.42511213 -0.48051417]\n",
      "  [-0.0810945  -0.38730472  0.8977728   2.2052171  -0.25967747\n",
      "    1.9531758  -1.272748   -0.11579217 -0.18856959 -0.25455534\n",
      "   -0.28446746 -0.18968877 -0.22868285 -0.2457605  -0.45643193\n",
      "   -0.2678875  -0.27539313 -0.21662965 -0.4035148 ]\n",
      "  [ 0.17782737 -0.3560871   0.655218    2.8048675  -0.49671292\n",
      "    1.2391412  -1.8322055   0.10485457 -0.00992225 -0.15297806\n",
      "   -0.06432448 -0.10866537 -0.09330364 -0.18075138 -0.26765507\n",
      "   -0.3410354  -0.2665863  -0.27105    -0.6046138 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "[[[ 1.24492097e+00 -1.93660438e-01  1.89816862e-01  3.86651587e+00\n",
      "   -6.05880857e-01  1.97132230e-01 -3.08562279e+00  1.52336821e-01\n",
      "    5.46795726e-02 -1.86903998e-02 -1.75186098e-02 -2.13626847e-02\n",
      "   -1.04307972e-01 -1.48671895e-01  5.47285490e-02 -1.13346487e-01\n",
      "   -2.69753098e-01 -3.18238407e-01 -3.58684540e-01]\n",
      "  [-4.36989546e-01 -3.55386615e-01  1.21508408e+00  9.43212867e-01\n",
      "    4.70128655e-03  2.95083427e+00 -4.98030521e-02 -5.17266810e-01\n",
      "   -4.97163981e-01 -4.36979175e-01 -6.48633897e-01 -4.35995072e-01\n",
      "   -4.58835334e-01 -3.04522574e-01 -6.76018834e-01 -2.72567838e-01\n",
      "   -2.28564978e-01  9.13128257e-02 -2.31914863e-01]\n",
      "  [ 1.79468140e-01 -2.70995080e-01  8.49405289e-01  3.12790775e+00\n",
      "   -7.16266274e-01  1.82897365e+00 -1.63462424e+00 -1.08364776e-01\n",
      "   -2.18353435e-01 -2.55858094e-01 -1.79560259e-01 -4.25493211e-01\n",
      "   -2.56773084e-01 -3.94987047e-01 -7.50278160e-02 -4.74460959e-01\n",
      "   -4.33861434e-01 -4.61391687e-01 -5.72416782e-01]\n",
      "  [-3.63840163e-01 -3.89502406e-01  1.13718641e+00  1.48423529e+00\n",
      "   -1.60415202e-01  2.84836745e+00 -4.51400965e-01 -4.40150023e-01\n",
      "   -4.03529227e-01 -4.17235345e-01 -5.41982830e-01 -3.86055917e-01\n",
      "   -4.36634928e-01 -4.33914006e-01 -5.54951906e-01 -3.98272246e-01\n",
      "   -2.95028210e-01 -1.60601258e-01 -3.28629017e-01]\n",
      "  [ 2.01378390e-03 -2.96551108e-01  9.13003087e-01  2.74038720e+00\n",
      "   -5.51564932e-01  2.10962677e+00 -1.40839946e+00 -1.47645012e-01\n",
      "   -2.60947198e-01 -2.85213917e-01 -2.22768530e-01 -4.24957603e-01\n",
      "   -3.11531991e-01 -4.44163263e-01 -2.09987655e-01 -4.62247998e-01\n",
      "   -4.07505333e-01 -4.86678839e-01 -4.97750521e-01]\n",
      "  [-2.36797348e-01 -3.86121362e-01  1.03305006e+00  1.83291829e+00\n",
      "   -1.98069155e-01  2.55499601e+00 -8.54486763e-01 -3.32758695e-01\n",
      "   -3.50702822e-01 -3.64881068e-01 -4.37447309e-01 -3.42393249e-01\n",
      "   -3.68677616e-01 -4.17618603e-01 -4.86252576e-01 -3.27019215e-01\n",
      "   -3.02657187e-01 -2.60905027e-01 -3.05065662e-01]\n",
      "  [-6.77450374e-03 -3.30665767e-01  8.52066398e-01  2.59152317e+00\n",
      "   -4.37416226e-01  1.98146915e+00 -1.45993745e+00 -1.14369288e-01\n",
      "   -2.42879435e-01 -2.72186756e-01 -2.19211623e-01 -3.42156738e-01\n",
      "   -2.65732825e-01 -3.80273700e-01 -2.73412883e-01 -3.61070901e-01\n",
      "   -3.64858359e-01 -4.31531906e-01 -4.58865523e-01]\n",
      "  [-5.31712659e-02 -3.92697126e-01  8.25372219e-01  2.11655641e+00\n",
      "   -2.65056044e-01  1.98195827e+00 -1.25096893e+00 -1.80165842e-01\n",
      "   -2.14821205e-01 -3.04508209e-01 -2.99629092e-01 -2.18329221e-01\n",
      "   -2.36798644e-01 -3.30149680e-01 -4.01260972e-01 -2.85597414e-01\n",
      "   -2.86627889e-01 -2.59932458e-01 -3.88709903e-01]\n",
      "  [ 1.68864623e-01 -3.63767207e-01  6.08976126e-01  2.61530137e+00\n",
      "   -4.54034418e-01  1.35759592e+00 -1.73272121e+00  1.89145617e-02\n",
      "   -5.78740984e-02 -2.14811921e-01 -1.06074020e-01 -1.44715279e-01\n",
      "   -1.16024218e-01 -2.70811945e-01 -2.44911626e-01 -3.42314988e-01\n",
      "   -2.88628638e-01 -3.13188463e-01 -5.66202760e-01]]]\n"
     ]
    }
   ],
   "source": [
    "yhatt = model.predict(yhat, verbose=1)\n",
    "print(yhatt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.1],\n",
       "        [0.2]],\n",
       "\n",
       "       [[0.3],\n",
       "        [0.4]],\n",
       "\n",
       "       [[0.5],\n",
       "        [0.6]],\n",
       "\n",
       "       [[0.7],\n",
       "        [0.8]],\n",
       "\n",
       "       [[0.9],\n",
       "        [0.2]],\n",
       "\n",
       "       [[0.3],\n",
       "        [0.4]],\n",
       "\n",
       "       [[0.5],\n",
       "        [0.6]],\n",
       "\n",
       "       [[0.7],\n",
       "        [0.8]],\n",
       "\n",
       "       [[0.9],\n",
       "        [1. ]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.2],\n",
       "        [0.3],\n",
       "        [0.4],\n",
       "        [0.5],\n",
       "        [0.6],\n",
       "        [0.7],\n",
       "        [0.8],\n",
       "        [0.9]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.1258\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1237\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1218\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1201\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1184\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1169\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1153\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1138\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1122\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1105\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1089\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1072\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1055\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1038\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1020\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1002\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0983\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0964\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0944\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0923\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0902\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0880\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0858\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0834\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0810\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0785\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0759\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0733\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0705\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0677\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "[0.08172032]\n"
     ]
    }
   ],
   "source": [
    "# lstm autoencoder reconstruct and predict sequence\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.utils import plot_model\n",
    "from keras import Sequential\n",
    "# define input sequence\n",
    "seq_in = np.array([[0.1, 0.2], [0.3, 0.4]])\n",
    "# reshape input into [samples, timesteps, features]\n",
    "n_in = len(seq_in)\n",
    "seq_in = seq_in.reshape((1, n_in, 2))\n",
    "# prepare output sequence\n",
    "seq_out = seq_in[:, 1:, :]\n",
    "n_out = n_in - 1\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_in,2)))\n",
    "model.add(RepeatVector(n_out))\n",
    "model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(2)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "plot_model(model, show_shapes=True, to_file='predict_lstm_autoencoder.png')\n",
    "# fit model\n",
    "model.fit(seq_in, seq_out, epochs=30, verbose=1)\n",
    "# demonstrate prediction\n",
    "yhat = model.predict(seq_in, verbose=1)\n",
    "print(yhat[0,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282\n",
      "160512 170240\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "for i, block in enumerate(stream.new()):\n",
    "    frames.append([i * hop_length, i * hop_length + frame_length])\n",
    "\n",
    "# each dict value has to be a 1D interlaved array, as Max dict object has trouble reading 2D arrays\n",
    "# start sample is always stored at an even index and is followed by end sample\n",
    "labelled_frames = dict()\n",
    "for label, frame in zip(labels, frames):\n",
    "    label = int(label)\n",
    "    # print(label.dtype)\n",
    "    if label not in labelled_frames:\n",
    "        labelled_frames[label] = []\n",
    "    labelled_frames[label].extend(frame) # use extend instead of append\n",
    "if verbose:    \n",
    "    print(len(labelled_frames[0])) # sanity check\n",
    "    print(labelled_frames[0][0], labelled_frames[0][1]) # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode\n",
    "# build a subsequence for every <step> frames\n",
    "# and a corresponding label that follows it\n",
    "features = [] # these will be features\n",
    "targets = [] # these will be targets\n",
    "for i in range(0, len(labels) - maxlen, step):\n",
    "    features.append(labels[i: i + maxlen])\n",
    "    targets.append(labels[i + maxlen])\n",
    "from keras.utils import to_categorical\n",
    "encoded_features = to_categorical(features, dtype =\"bool\")\n",
    "encoded_targets = to_categorical(targets, dtype =\"bool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(879, 78, 78)\n",
      "(879, 78)\n",
      "Model: \"trebles-8th\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 78, 78)]          0         \n",
      "                                                                 \n",
      " gru_6 (GRU)                 (None, 78, 24)            7488      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 78, 78)            1950      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,438\n",
      "Trainable params: 9,438\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "22/22 [==============================] - 2s 38ms/step - loss: 4.2769 - accuracy: 0.3349 - val_loss: 4.1757 - val_accuracy: 0.8195\n",
      "Epoch 2/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 4.0282 - accuracy: 0.9541 - val_loss: 3.8608 - val_accuracy: 0.9872\n",
      "Epoch 3/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 3.6335 - accuracy: 0.9872 - val_loss: 3.3767 - val_accuracy: 0.9872\n",
      "Epoch 4/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 3.0741 - accuracy: 0.9872 - val_loss: 2.7378 - val_accuracy: 0.9872\n",
      "Epoch 5/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 2.3465 - accuracy: 0.9872 - val_loss: 1.9411 - val_accuracy: 0.9872\n",
      "Epoch 6/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 1.5644 - accuracy: 0.9872 - val_loss: 1.1996 - val_accuracy: 0.9872\n",
      "Epoch 7/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.9049 - accuracy: 0.9872 - val_loss: 0.6675 - val_accuracy: 0.9872\n",
      "Epoch 8/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.5108 - accuracy: 0.9872 - val_loss: 0.3958 - val_accuracy: 0.9872\n",
      "Epoch 9/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.3214 - accuracy: 0.9872 - val_loss: 0.2710 - val_accuracy: 0.9872\n",
      "Epoch 10/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.2402 - accuracy: 0.9872 - val_loss: 0.2229 - val_accuracy: 0.9872\n",
      "Epoch 11/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.2046 - accuracy: 0.9872 - val_loss: 0.1944 - val_accuracy: 0.9872\n",
      "Epoch 12/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.1882 - accuracy: 0.9872 - val_loss: 0.1807 - val_accuracy: 0.9872\n",
      "Epoch 13/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.1665 - accuracy: 0.9872 - val_loss: 0.1585 - val_accuracy: 0.9872\n",
      "Epoch 14/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.1507 - accuracy: 0.9872 - val_loss: 0.1452 - val_accuracy: 0.9872\n",
      "Epoch 15/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.1433 - accuracy: 0.9872 - val_loss: 0.1386 - val_accuracy: 0.9872\n",
      "Epoch 16/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.1330 - accuracy: 0.9872 - val_loss: 0.1289 - val_accuracy: 0.9872\n",
      "Epoch 17/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.1271 - accuracy: 0.9872 - val_loss: 0.1263 - val_accuracy: 0.9872\n",
      "Epoch 18/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.1273 - accuracy: 0.9872 - val_loss: 0.1254 - val_accuracy: 0.9872\n",
      "Epoch 19/100\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.1206 - accuracy: 0.9872 - val_loss: 0.1155 - val_accuracy: 0.9872\n",
      "Epoch 20/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.1135 - accuracy: 0.9872 - val_loss: 0.1105 - val_accuracy: 0.9872\n",
      "Epoch 21/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.1068 - accuracy: 0.9872 - val_loss: 0.1027 - val_accuracy: 0.9872\n",
      "Epoch 22/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.0996 - accuracy: 0.9872 - val_loss: 0.0963 - val_accuracy: 0.9872\n",
      "Epoch 23/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.0943 - accuracy: 0.9872 - val_loss: 0.0938 - val_accuracy: 0.9872\n",
      "Epoch 24/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.0927 - accuracy: 0.9872 - val_loss: 0.0909 - val_accuracy: 0.9872\n",
      "Epoch 25/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.0883 - accuracy: 0.9872 - val_loss: 0.0863 - val_accuracy: 0.9872\n",
      "Epoch 26/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.0851 - accuracy: 0.9872 - val_loss: 0.0860 - val_accuracy: 0.9872\n",
      "Epoch 27/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.0859 - accuracy: 0.9872 - val_loss: 0.0870 - val_accuracy: 0.9872\n",
      "Epoch 28/100\n",
      "22/22 [==============================] - 0s 21ms/step - loss: 0.0847 - accuracy: 0.9872 - val_loss: 0.0843 - val_accuracy: 0.9872\n",
      "Epoch 29/100\n",
      "22/22 [==============================] - 0s 20ms/step - loss: 0.0820 - accuracy: 0.9872 - val_loss: 0.0820 - val_accuracy: 0.9872\n",
      "Epoch 30/100\n",
      "22/22 [==============================] - 0s 19ms/step - loss: 0.0811 - accuracy: 0.9872 - val_loss: 0.0819 - val_accuracy: 0.9872\n",
      "Epoch 31/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.0796 - accuracy: 0.9872 - val_loss: 0.0797 - val_accuracy: 0.9872\n",
      "Epoch 32/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.0784 - accuracy: 0.9872 - val_loss: 0.0801 - val_accuracy: 0.9872\n",
      "Epoch 33/100\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.0785 - accuracy: 0.9872 - val_loss: 0.0799 - val_accuracy: 0.9872\n",
      "Epoch 34/100\n",
      "22/22 [==============================] - 0s 18ms/step - loss: 0.0792 - accuracy: 0.9872 - val_loss: 0.0811 - val_accuracy: 0.9872\n",
      "Saving model to: models/trebles-8th\n",
      "Saved model to: models/trebles-8th/trebles-8th.keras\n",
      "Saved frames to: models/trebles-8th/trebles-8th_frames.json\n",
      "Saved config to: models/trebles-8th/trebles-8th_config.json\n",
      "Total epochs : 34\n",
      "0.08107524365186691\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# one-hot encode features and targets\n",
    "# adapted from wandb character generation code referenced at the beginning of this notebook\n",
    "# encoded_features = np.zeros((len(features), maxlen, n_classes), dtype=bool)\n",
    "# encoded_targets = np.zeros((len(targets), n_classes), dtype=bool)\n",
    "# for i, sequence in enumerate(features):\n",
    "#     # print(i, sequence)\n",
    "#     for t, label in enumerate(sequence):\n",
    "#         encoded_features[i, t, label] = 1\n",
    "#         # print(encoded_features[i, t])\n",
    "#     encoded_targets[i, targets[i]] = 1\n",
    "# sanity check\n",
    "if verbose:\n",
    "    print(encoded_features.shape)\n",
    "    print(encoded_targets.shape)\n",
    "\n",
    "inputs = Input(shape=(maxlen, n_classes))\n",
    "x = GRU(hidden_units, return_sequences=True)(inputs)\n",
    "# according to DLWP [13.2.1] softmax tends to be unstable in float16\n",
    "outputs = Dense(n_classes, activation='softmax', dtype=\"float32\")(x)\n",
    "callback = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model._name = name\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', # since we are using integer labels\n",
    "    optimizer=\"adam\",\n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(\n",
    "    encoded_features,\n",
    "    encoded_targets,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=verbose,\n",
    "    validation_split=validation_split,\n",
    "    callbacks=[callback]\n",
    ")\n",
    "\n",
    "path = Path(directory + \"/models/\" + name)\n",
    "print(f\"Saving model to: {path}\")\n",
    "path.mkdir(exist_ok=True, parents=True)\n",
    "model_path = path / (name + \".keras\")\n",
    "model.save(model_path)\n",
    "d_path = path / (name + \"_frames.json\")\n",
    "d_path.write_text(json.dumps(labelled_frames, cls=NumpyEncoder))\n",
    "config = dict()\n",
    "config[\"filename\"] = audio_path.split('/')[-1]\n",
    "config[\"sr\"] = sr\n",
    "config[\"BPM\"] = BPM\n",
    "config[\"beat\"] = beat\n",
    "config[\"n_classes\"] = int(n_classes)\n",
    "config[\"maxlen\"] = int(maxlen)\n",
    "config[\"onset_detection\"] = False\n",
    "config[\"hop_length\"] = hop_length\n",
    "config[\"frame_length\"] = frame_length\n",
    "config[\"block_length\"] = block_length\n",
    "c_path = path / (name + \"_config.json\")    \n",
    "c_path.write_text(json.dumps(config))\n",
    "if verbose:\n",
    "    print(f\"Saved model to: {model_path}\")\n",
    "    print(f\"Saved frames to: {d_path}\")\n",
    "    print(f\"Saved config to: {c_path}\")\n",
    "print(f\"Total epochs : {len(history.history['loss'])}\")\n",
    "print(history.history['val_loss'][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
